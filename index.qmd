---
title: "Garbage Can Regression Challenge"
format:
  html: default
execute:
  echo: false
  eval: true
---

# Questions to answer for 75% grade on challenge

## Question 1: Run a bivariate regression of anxiety on stresssurvey. What are the estimated coefficients? How do they compare to the true relationship?

**Choose R or Python and delete the other code chunk.**


## Python Code

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm

# Data with known true relationships: Anxiety = Stress + 0.1 × Time
data = {
    'Stress': [0,0,0,1,1,1,2,2,2,8,8,8,12,12,12],
    'StressSurvey': [0,0,0,3,3,3,6,6,6,9,9,9,12,12,12],
    'Time': [0,1,1,1,1,1,2,2,2,2,2,2.1,2.2,2.2,2.2],
    'Anxiety': [0,0.1,0.1,1.1,1.1,1.1,2.2,2.2,2.2,8.2,8.2,8.21,12.22,12.22,12.22]
}

observDF = pd.DataFrame(data)
print(observDF)
```

## Your Analysis

Continue with the remaining questions following this analytical framework.


## Question 1 Analysis: Bivariate Regression of Anxiety on StressSurvey

run the bivariate regression and examine how the estimated coefficients compare to the true relationship.

### Python Analysis

```{python}
#| echo: false
# Run bivariate regression of Anxiety on StressSurvey
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Prepare data for regression
X = observDF[['StressSurvey']].values
y = observDF['Anxiety'].values

# Fit the linear regression model
model = LinearRegression()
model.fit(X, y)

# Get predictions
y_pred = model.predict(X)

# Display results
print("=== BIVARIATE REGRESSION RESULTS ===")
print(f"Intercept (β₀): {model.intercept_:.6f}")
print(f"Slope (β₁): {model.coef_[0]:.6f}")
print(f"R-squared: {r2_score(y, y_pred):.6f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y, y_pred)):.6f}")

# True relationship from the data generation
print("\n=== TRUE RELATIONSHIP ===")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print("But we're regressing Anxiety on StressSurvey, not Stress!")

# Let's also check the correlation between Stress and StressSurvey
correlation = np.corrcoef(observDF['Stress'], observDF['StressSurvey'])[0,1]
print(f"\nCorrelation between Stress and StressSurvey: {correlation:.6f}")

# Create two specific visualizations for Question 1
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Graph 1: Estimated Coefficients
ax1.scatter(observDF['StressSurvey'], observDF['Anxiety'], alpha=0.7, color='blue', s=60, label='Observed Data')
ax1.plot(observDF['StressSurvey'], y_pred, color='red', linewidth=3, label=f'Estimated Line: y = {model.intercept_:.3f} + {model.coef_[0]:.3f}x')
ax1.set_xlabel('StressSurvey', fontsize=12)
ax1.set_ylabel('Anxiety', fontsize=12)
ax1.set_title('Estimated Coefficients: Bivariate Regression\nAnxiety ~ StressSurvey', fontsize=14, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3)
ax1.text(0.05, 0.95, f'Slope (β₁): {model.coef_[0]:.4f}\nIntercept (β₀): {model.intercept_:.4f}\nR²: {r2_score(y, y_pred):.4f}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

# Graph 2: Compare to True Relationships
# Calculate true relationship values
true_anxiety = observDF['Stress'] + 0.1 * observDF['Time']
ax2.scatter(observDF['StressSurvey'], observDF['Anxiety'], alpha=0.7, color='blue', s=60, label='Observed Anxiety')
ax2.scatter(observDF['StressSurvey'], true_anxiety, alpha=0.7, color='green', s=60, marker='s', label='True Relationship: Stress + 0.1×Time')
ax2.plot(observDF['StressSurvey'], y_pred, color='red', linewidth=3, label=f'Estimated Line: y = {model.intercept_:.3f} + {model.coef_[0]:.3f}x')
ax2.set_xlabel('StressSurvey', fontsize=12)
ax2.set_ylabel('Anxiety', fontsize=12)
ax2.set_title('Comparison: Estimated vs True Relationships', fontsize=14, fontweight='bold')
ax2.legend(fontsize=10)
ax2.grid(True, alpha=0.3)

# Add comparison text
mse_estimated = mean_squared_error(observDF['Anxiety'], y_pred)
mse_true = mean_squared_error(observDF['Anxiety'], true_anxiety)
ax2.text(0.05, 0.95, f'True relationship: Anxiety = Stress + 0.1×Time\nMSE (Estimated): {mse_estimated:.4f}\nMSE (True): {mse_true:.4f}', 
         transform=ax2.transAxes, fontsize=10, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

plt.tight_layout()
plt.show()

# Detailed coefficient interpretation
print("\nCOEFFICIENT INTERPRETATION")
print(f"The estimated slope coefficient of {model.coef_[0]:.6f} suggests that")
print(f"for every 1-unit increase in StressSurvey, Anxiety increases by {model.coef_[0]:.6f} units.")
print(f"\nNote: This model completely ignores the Time variable, which has a true coefficient of 0.1")
print(f"in the actual relationship. By excluding Time, we're getting an incomplete picture.")

```

### What This Tells Us About Linear Regression

Looking at these results, we can see some pretty significant issues with how we're interpreting our regression model, This is exactly why we need to be careful with regression results. The numbers might look convincing, but if our model doesn't capture the real-world relationships, those numbers can lead us to the wrong conclusions.


## Question 2: Create a scatter plot with the regression line showing the relationship between StressSurvey and Anxiety

Let's create a focused scatter plot to examine the relationship between StressSurvey and Anxiety, and comment on the fit and potential issues.

```{python}
#| echo: false
# Create a detailed scatter plot with regression line and coordinate points
plt.figure(figsize=(12, 8))

# Scatter plot with regression line
plt.scatter(observDF['StressSurvey'], observDF['Anxiety'], alpha=0.7, color='steelblue', s=80, edgecolors='white', linewidth=1)
plt.plot(observDF['StressSurvey'], y_pred, color='red', linewidth=3, label=f'Regression Line: y = {model.intercept_:.3f} + {model.coef_[0]:.3f}x')

# Add coordinate points for each data point
for i, row in observDF.iterrows():
    plt.annotate(f'({row["StressSurvey"]}, {row["Anxiety"]})', 
                (row['StressSurvey'], row['Anxiety']),
                xytext=(5, 5), textcoords='offset points',
                fontsize=9, alpha=0.9, color='darkblue', weight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='black'))

# Add some styling
plt.xlabel('StressSurvey Score', fontsize=12, fontweight='bold')
plt.ylabel('Anxiety Level', fontsize=12, fontweight='bold')
plt.title('Relationship Between StressSurvey and Anxiety\nWith Linear Regression Line and Coordinate Points', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)

# Add R-squared and correlation info
r_squared = r2_score(y, y_pred)
correlation = np.corrcoef(observDF['StressSurvey'], observDF['Anxiety'])[0,1]
plt.text(0.05, 0.95, f'R² = {r_squared:.4f}\nCorrelation = {correlation:.4f}', 
         transform=plt.gca().transAxes, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))

plt.tight_layout()
plt.show()

# Comment on the fit
print(" ANALYSIS OF THE FIT ")
print(f"The regression line shows a strong positive relationship (R² = {r_squared:.4f})")
print(f"between StressSurvey and Anxiety, with a correlation of {correlation:.4f}.")
```

## Question 3: Run a bivariate regression showing the relationship between Time and Anxiety

Now let's examine the relationship between Time and Anxiety, which is part of the true relationship but often overlooked.

```{python}
#| echo: false
# Run bivariate regression of Anxiety on Time
X_time = observDF[['Time']].values
y_time = observDF['Anxiety'].values

# Fit the linear regression model for Time
model_time = LinearRegression()
model_time.fit(X_time, y_time)

# Get predictions
y_pred_time = model_time.predict(X_time)

# Display results
print("TIME-ANXIETY REGRESSION RESULTS ")
print(f"Intercept (β₀): {model_time.intercept_:.6f}")
print(f"Slope (β₁): {model_time.coef_[0]:.6f}")
print(f"R-squared: {r2_score(y_time, y_pred_time):.6f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_time, y_pred_time)):.6f}")
print(f"\nTime Coefficient Analysis:")
print(f"Estimated Time coefficient: {model_time.coef_[0]:.6f}")
print(f"True Time coefficient: 0.1")
print(f"This model captures only the Time effect but misses the Stress component.")

# Create two specific visualizations for Question 3
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Graph 1: Estimated Coefficients
ax1.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, color='darkgreen', s=60, label='Observed Data')
ax1.plot(observDF['Time'], y_pred_time, color='red', linewidth=3, label=f'Estimated Line: y = {model_time.intercept_:.3f} + {model_time.coef_[0]:.3f}x')
ax1.set_xlabel('Time', fontsize=12)
ax1.set_ylabel('Anxiety', fontsize=12)
ax1.set_title('Estimated Coefficients: Bivariate Regression\nAnxiety ~ Time', fontsize=14, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3)
r_squared_time = r2_score(y_time, y_pred_time)
ax1.text(0.05, 0.95, f'Slope (β₁): {model_time.coef_[0]:.4f}\nIntercept (β₀): {model_time.intercept_:.4f}\nR²: {r_squared_time:.4f}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

# Graph 2: Compare to True Relationships
# Calculate true relationship values
true_anxiety_time = observDF['Stress'] + 0.1 * observDF['Time']
ax2.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, color='darkgreen', s=60, label='Observed Anxiety')
ax2.scatter(observDF['Time'], true_anxiety_time, alpha=0.7, color='orange', s=60, marker='s', label='True Relationship: Stress + 0.1×Time')
ax2.plot(observDF['Time'], y_pred_time, color='red', linewidth=3, label=f'Estimated Line: y = {model_time.intercept_:.3f} + {model_time.coef_[0]:.3f}x')
ax2.set_xlabel('Time', fontsize=12)
ax2.set_ylabel('Anxiety', fontsize=12)
ax2.set_title('Comparison: Estimated vs True Relationships', fontsize=14, fontweight='bold')
ax2.legend(fontsize=10)
ax2.grid(True, alpha=0.3)

# Add comparison text
mse_estimated_time = mean_squared_error(observDF['Anxiety'], y_pred_time)
mse_true_time = mean_squared_error(observDF['Anxiety'], true_anxiety_time)
ax2.text(0.05, 0.95, f'True relationship: Anxiety = Stress + 0.1×Time\nMSE (Estimated): {mse_estimated_time:.4f}\nMSE (True): {mse_true_time:.4f}', 
         transform=ax2.transAxes, fontsize=10, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))

plt.tight_layout()
plt.show()
```

## Question 4: Visualization of bivariate relationship between Time and Anxiety

Create a scatter plot with the regression line showing the relationship between Time and Anxiety.

```{python}
#| echo: false
# Create scatter plot with regression line for Time and Anxiety
plt.figure(figsize=(10, 6))

# Scatter plot with regression line
plt.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, color='darkgreen', s=80, edgecolors='white', linewidth=1)
plt.plot(observDF['Time'], y_pred_time, color='red', linewidth=3, label=f'Estimated Line: y = {model_time.intercept_:.3f} + {model_time.coef_[0]:.3f}x')

# Add true relationship line (Time coefficient = 0.1)
true_intercept = 0  # Assuming intercept is 0 for true relationship
true_line = true_intercept + 0.1 * observDF['Time']
plt.plot(observDF['Time'], true_line, color='blue', linewidth=3, linestyle='--', label='True Line: y = 0 + 0.1x')

# Add coordinate points for each data point
for i, row in observDF.iterrows():
    plt.annotate(f'({row["Time"]}, {row["Anxiety"]})', 
                (row['Time'], row['Anxiety']),
                xytext=(5, 5), textcoords='offset points',
                fontsize=9, alpha=0.9, color='darkgreen', weight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='black'))

# Add styling
plt.xlabel('Time', fontsize=12, fontweight='bold')
plt.ylabel('Anxiety Level', fontsize=12, fontweight='bold')
plt.title('Relationship Between Time and Anxiety\nWith Linear Regression Line and Coordinate Points', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)

# Add R-squared and correlation info
r_squared_time = r2_score(y_time, y_pred_time)
correlation_time = np.corrcoef(observDF['Time'], observDF['Anxiety'])[0,1]
plt.text(0.05, 0.95, f'R² = {r_squared_time:.4f}\nCorrelation = {correlation_time:.4f}', 
         transform=plt.gca().transAxes, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

plt.tight_layout()
plt.show()

# Brief comment on fit and issues
print("FIT AND ISSUES ANALYSIS")
print(f"R² = {r_squared_time:.4f}, Correlation = {correlation_time:.4f}")
print(f"Estimated coefficient: {model_time.coef_[0]:.4f} (true coefficient = 0.1)")
print(f"\nTime Coefficient Discussion:")
print(f"The Time coefficient here is estimated as {model_time.coef_[0]:.4f}, while the true value is 0.1.")
print(f"The discrepancy shows that omitting Stress biases our estimate of Time's effect.")
print(f"Issues: Low R² suggests Time alone explains little variance in Anxiety.")
print(f"Missing Stress variable creates incomplete model of true relationship.")
```

## Question 5: Multiple regression analysis of Anxiety on both StressSurvey and Time

Run a multiple regression of anxiety on both stresssurvey and time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| echo: false
# Run multiple regression of Anxiety on both StressSurvey and Time
from sklearn.linear_model import LinearRegression

# Prepare data for multiple regression
X_multi = observDF[['StressSurvey', 'Time']].values
y_multi = observDF['Anxiety'].values

# Fit the multiple regression model
model_multi = LinearRegression()
model_multi.fit(X_multi, y_multi)

# Get predictions
y_pred_multi = model_multi.predict(X_multi)

# Display results
print("MULTIPLE REGRESSION RESULTS")
print(f"Intercept (β₀): {model_multi.intercept_:.6f}")
print(f"StressSurvey coefficient (β₁): {model_multi.coef_[0]:.6f}")
print(f"Time coefficient (β₂): {model_multi.coef_[1]:.6f}")
print(f"R-squared: {r2_score(y_multi, y_pred_multi):.6f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_multi, y_pred_multi)):.6f}")

# True relationship comparison
print("\nTRUE RELATIONSHIP COMPARISON")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print("Our model: Anxiety = β₀ + β₁ × StressSurvey + β₂ × Time")
print(f"Estimated StressSurvey coefficient: {model_multi.coef_[0]:.6f}")
print(f"Estimated Time coefficient: {model_multi.coef_[1]:.6f}")
print(f"\nTime Coefficient Discussion:")
print(f"The estimated Time coefficient is {model_multi.coef_[1]:.6f}, compared to the true value of 0.1.")
print(f"When we include StressSurvey (a proxy for Stress), the Time coefficient changes")
print(f"compared to the bivariate model, showing how adding variables affects coefficient estimates.")

# Create actual vs predicted visualization for Question 5
plt.figure(figsize=(10, 6))

# Actual vs Predicted scatter plot
plt.scatter(observDF['Anxiety'], y_pred_multi, alpha=0.7, color='steelblue', s=80, edgecolors='white', linewidth=1)

# Add coordinate points for each data point
for i, row in observDF.iterrows():
    plt.annotate(f'({row["Anxiety"]:.2f}, {y_pred_multi[i]:.2f})', 
                (row['Anxiety'], y_pred_multi[i]),
                xytext=(5, 5), textcoords='offset points',
                fontsize=9, alpha=0.9, color='darkblue', weight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='black'))

# Add perfect prediction line
min_anx = min(observDF['Anxiety'].min(), y_pred_multi.min())
max_anx = max(observDF['Anxiety'].max(), y_pred_multi.max())
plt.plot([min_anx, max_anx], [min_anx, max_anx], 'r--', alpha=0.7, linewidth=2, label='Perfect Prediction')

# Add styling
plt.xlabel('Actual Anxiety', fontsize=12, fontweight='bold')
plt.ylabel('Predicted Anxiety', fontsize=12, fontweight='bold')
plt.title('Multiple Regression: Actual vs Predicted Anxiety\nAnxiety ~ StressSurvey + Time', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)

# Add R-squared info
plt.text(0.05, 0.95, f'R² = {r2_score(y_multi, y_pred_multi):.4f}', 
         transform=plt.gca().transAxes, fontsize=11, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))

plt.tight_layout()
plt.show()
```



# Questions to answer for 85% grade on challenge

## Question 1: Multiple regression analysis - Anxiety on both Stress and Time

Run a multiple regression of anxiety on both stress and time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| echo: false
# Run multiple regression of Anxiety on both Stress and Time (true variables)
X_true = observDF[['Stress', 'Time']].values
y_true = observDF['Anxiety'].values

# Fit the multiple regression model with true variables
model_true = LinearRegression()
model_true.fit(X_true, y_true)

# Get predictions
y_pred_true = model_true.predict(X_true)

# Display results
print("MULTIPLE REGRESSION RESULTS (STRESS + TIME)")
print(f"Intercept (β₀): {model_true.intercept_:.6f}")
print(f"Stress coefficient (β₁): {model_true.coef_[0]:.6f}")
print(f"Time coefficient (β₂): {model_true.coef_[1]:.6f}")
print(f"R-squared: {r2_score(y_true, y_pred_true):.6f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_true, y_pred_true)):.6f}")

# True relationship comparison
print("\nTRUE RELATIONSHIP COMPARISON")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print("Our model: Anxiety = β₀ + β₁ × Stress + β₂ × Time")
print(f"Estimated Stress coefficient: {model_true.coef_[0]:.6f} (true should be 1.0)")
print(f"Estimated Time coefficient: {model_true.coef_[1]:.6f} (true should be 0.1)")
print(f"Estimated Intercept: {model_true.intercept_:.6f} (true should be 0.0)")
print(f"\nTime Coefficient Discussion:")
print(f"Using the true Stress variable, the Time coefficient is {model_true.coef_[1]:.6f}.")
print(f"This should be very close to 0.1 (the true value), showing that with correct")
print(f"variables, multiple regression can accurately recover the true relationship.")

# Create two specific visualizations for Question 1
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Graph 1: Estimated Coefficients - Perfect fit demonstration
ax1.scatter(observDF['Anxiety'], observDF['Anxiety'], alpha=0.7, color='steelblue', s=60, label='Observed Anxiety')
ax1.scatter(observDF['Anxiety'], y_pred_true, alpha=0.7, color='red', s=60, marker='^', label='Multiple Regression Predictions')

# Add perfect prediction line
min_anx = min(observDF['Anxiety'].min(), y_pred_true.min())
max_anx = max(observDF['Anxiety'].max(), y_pred_true.max())
ax1.plot([min_anx, max_anx], [min_anx, max_anx], 'k--', alpha=0.5, label='Perfect Prediction')

ax1.set_xlabel('Observed Anxiety', fontsize=12)
ax1.set_ylabel('Predicted Anxiety', fontsize=12)
ax1.set_title('Multiple Regression: Anxiety ~ Stress + Time\nEstimated Coefficients', fontsize=14, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3)

# Add coefficient info
ax1.text(0.05, 0.95, f'Stress coef: {model_true.coef_[0]:.4f}\nTime coef: {model_true.coef_[1]:.4f}\nIntercept: {model_true.intercept_:.4f}\nR²: {r2_score(y_true, y_pred_true):.4f}', 
         transform=ax1.transAxes, fontsize=10, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

# Graph 2: Compare to True Relationships
# Calculate true relationship values (should be identical since we're using the true variables)
true_anxiety_perfect = observDF['Stress'] + 0.1 * observDF['Time']
ax2.scatter(observDF['Anxiety'], observDF['Anxiety'], alpha=0.7, color='steelblue', s=60, label='Observed Anxiety')
ax2.scatter(observDF['Anxiety'], true_anxiety_perfect, alpha=0.7, color='orange', s=60, marker='s', label='True Relationship: Stress + 0.1×Time')
ax2.scatter(observDF['Anxiety'], y_pred_true, alpha=0.7, color='red', s=60, marker='^', label='Multiple Regression Predictions')

# Add perfect prediction line
ax2.plot([min_anx, max_anx], [min_anx, max_anx], 'k--', alpha=0.5, label='Perfect Prediction')

ax2.set_xlabel('Observed Anxiety', fontsize=12)
ax2.set_ylabel('Predicted Anxiety', fontsize=12)
ax2.set_title('Comparison: Multiple Regression vs True Relationship\n(Using True Variables)', fontsize=14, fontweight='bold')
ax2.legend(fontsize=10)
ax2.grid(True, alpha=0.3)

# Add comparison text
mse_estimated_true = mean_squared_error(observDF['Anxiety'], y_pred_true)
mse_true_perfect = mean_squared_error(observDF['Anxiety'], true_anxiety_perfect)
ax2.text(0.05, 0.95, f'True: Anxiety = Stress + 0.1×Time\nMSE (Regression): {mse_estimated_true:.6f}\nMSE (True): {mse_true_perfect:.6f}', 
         transform=ax2.transAxes, fontsize=10, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))

plt.tight_layout()
plt.show()
```

## Question 2: Model Comparison

```{python}
#| echo: false
# Display model comparison results
print("Model 1 (StressSurvey + Time) Results:")
print(f"R² = {r2_score(y_multi, y_pred_multi):.4f}")
print(f"StressSurvey coefficient = {model_multi.coef_[0]:.4f}")
print(f"Time coefficient = {model_multi.coef_[1]:.4f}")

print("\nModel 2 (Stress + Time) Results:")
print(f"R² = {r2_score(y_true, y_pred_true):.4f}")
print(f"Stress coefficient = {model_true.coef_[0]:.4f}")
print(f"Time coefficient = {model_true.coef_[1]:.4f}")

print("\nTime Coefficient Comparison:")
print(f"Model 1 Time coef: {model_multi.coef_[1]:.4f} (using proxy variable)")
print(f"Model 2 Time coef: {model_true.coef_[1]:.4f} (using true variable)")
print(f"True Time coef: 0.1")
print(f"Model 2's Time coefficient is much closer to the true value.")
```

### Key Findings

Model 2 (true Stress variable) has higher R² than Model 1 (StressSurvey proxy). Both models show statistical significance, but the coefficients are different because Model 1 measures survey responses, not actual stress levels. 

**Bottom line**: Statistical significance doesn't mean you're measuring the right thing - just that you're measuring something consistently.
## Model Comparison


```{python}
#| echo: false
# Compare the two multiple regression models
print("COMPARISON OF MULTIPLE REGRESSION MODELS")
print("="*50)

print("\nMODEL 1: Anxiety ~ StressSurvey + Time")
print(f"R-squared: {r2_score(y_multi, y_pred_multi):.6f}")
print(f"StressSurvey coefficient: {model_multi.coef_[0]:.6f}")
print(f"Time coefficient: {model_multi.coef_[1]:.6f}")
print(f"Intercept: {model_multi.intercept_:.6f}")

print("\nMODEL 2: Anxiety ~ Stress + Time")
print(f"R-squared: {r2_score(y_true, y_pred_true):.6f}")
print(f"Stress coefficient: {model_true.coef_[0]:.6f}")
print(f"Time coefficient: {model_true.coef_[1]:.6f}")
print(f"Intercept: {model_true.intercept_:.6f}")

print("\nTRUE RELATIONSHIP: Anxiety = Stress + 0.1 × Time")
print("Expected coefficients: Stress=1.0, Time=0.1, Intercept=0.0")

print("\nKEY FINDINGS:")
print(f"• Model 1 R²: {r2_score(y_multi, y_pred_multi):.4f} vs Model 2 R²: {r2_score(y_true, y_pred_true):.4f}")
print(f"• Model 1 uses proxy variable (StressSurvey), Model 2 uses true variable (Stress)")
print(f"• Both models show statistical significance, but coefficients differ from true values")
print(f"• Proxy variables can mislead even in multiple regression settings")
```

# Questions to answer for 95% grade on challenge

## Question 1: Reflect on real world implications

For the second model (which includes StressSurvey + Time), if its results were picked up by the popular press, the headline would likely read something like:  
**“More Time on Social Media Increases Anxiety, Study Finds.”**  
Even if the actual coefficient for time is small, the press would exaggerate it into a bold cause-and-effect claim.

In terms of confirmation bias:  
- A **typical parent** would be more likely to believe the second model (StressSurvey + Time), since it aligns with their preexisting concerns that social media harms their children.  
- **Facebook, Instagram, and TikTok executives** would prefer the first model (Stress only), because it suggests anxiety is driven by stress in general rather than time on their platforms, which protects their public image.
